{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e50e1a-57d8-4324-96d6-b5281166784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8bcd7f-edca-4355-a4b6-6557af756b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I Love my dog\",\n",
    "    \"I Love my cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433cbf08-8912-4b9d-b044-4188323144da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Love my dog', 'I Love my cat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b2819e-ebc1-45d2-ae2d-ce0df6d808f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4629a32a-08d0-426a-850c-4fc3f3307fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade6e782-fd2d-4896-9c45-3193d3a59b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564f2a69-b3e9-4c98-98f4-4a8d6467783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18c424-b3c3-48fb-84b9-f2a394e30204",
   "metadata": {},
   "source": [
    "# Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b012b0-594b-4a94-845d-21db3d175185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1b9b17-d818-4983-92eb-2afda3c153cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I Love my dog\",\n",
    "    \"I Love my cat\",\n",
    "    \"You Love my dog very much\",\n",
    "    \"Do you think my dog is awesome\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd148326-e771-459f-98bf-4b83e49bc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f17868d6-5c11-4668-b4e8-d8f00258f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eaa6c82-61be-439f-981b-f73dc03ef132",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "110152d8-1237-4284-8e90-369586adc6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'love': 2,\n",
       " 'dog': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " 'cat': 6,\n",
       " 'very': 7,\n",
       " 'much': 8,\n",
       " 'do': 9,\n",
       " 'think': 10,\n",
       " 'is': 11,\n",
       " 'awesome': 12}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e109b8-4143-479c-8147-605f38762556",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76d5660c-4acf-4b3f-b0a2-81c77cf06745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Love my dog',\n",
       " 'I Love my cat',\n",
       " 'You Love my dog very much',\n",
       " 'Do you think my dog is awesome']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2262d5ea-79c9-49fb-8a74-505bb6964802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3, 7, 8], [9, 5, 10, 1, 3, 11, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41e82313-9dff-4c1e-8436-99b112705f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"i really love my dog\",\n",
    "    \"my dog love my brother\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc7cbfc1-f13b-4c79-af83-d093fc7653e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i really love my dog', 'my dog love my brother']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c197cd77-1ff0-4c3a-bb5c-3c8a46782ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c0814f-74d4-496e-a0ca-3ccd13b249c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [1, 3, 2, 1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9d786-c6ee-4a3d-9627-ce2acb36fb50",
   "metadata": {},
   "source": [
    "# Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2969c66e-67f5-42a3-a32c-88037c893c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07c6c9aa-cb22-4ba5-bc7b-9211cef315d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I Love my dog\",\n",
    "    \"I Love my cat\",\n",
    "    \"You Love my dog very much\",\n",
    "    \"Do you think my dog is awesome\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c54ca5e6-3c33-4164-9918-36a6446dc047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Love my dog',\n",
       " 'I Love my cat',\n",
       " 'You Love my dog very much',\n",
       " 'Do you think my dog is awesome']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d961764-7921-468d-9700-d94c3046f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100,oov_token=\"<oov>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd6771-c229-47b5-8e28-ae3c91716234",
   "metadata": {},
   "source": [
    "### tokenization is the process of breaking down a text into smaller units called \"tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87609e1d-b28c-4d32-b795-71c40ee8b4a1",
   "metadata": {},
   "source": [
    "### num_words=100 = Limits the vocabulary size to the top 100 most frequent words in the training data. Less frequent words will be treated as out-of-vocabulary (OOV) tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888e55b-02f5-47cb-a19f-bec10899043c",
   "metadata": {},
   "source": [
    "### OOV = This token will be assigned to words that are not present in the top 100 vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa91aa5-1d4d-4e53-a718-c1fa2e698aa6",
   "metadata": {},
   "source": [
    "### When the tokenizer encounters a word that is not in its vocabulary, it will replace it with the 'oov' token.This is very important, because it allows the model to handle words that were not present in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f608c3-f1c1-4efe-80da-67ce7e98c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4157350b-ae45-411a-af06-e7842b6d9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23972e86-f56d-49c8-8fc2-706d864fb99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<oov>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'very': 8,\n",
       " 'much': 9,\n",
       " 'do': 10,\n",
       " 'think': 11,\n",
       " 'is': 12,\n",
       " 'awesome': 13}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57115914-2f57-460f-884f-813dfe0a07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75b9f664-d2f7-442c-9118-09dc08d259a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4, 8, 9], [10, 6, 11, 2, 4, 12, 13]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd513ab8-fd2e-4b19-a33d-f2c3bd0b55ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i really love my dog', 'my dog love my brother']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1527a421-db6e-4ad2-83ba-9539a662f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1ce26fc-cf27-43a2-a534-0eb426c8494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 3, 2, 4], [2, 4, 3, 2, 1]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60772bd8-df7c-4c08-b16a-cecfbe91c502",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae8dac-3a50-43d3-a181-5ba567723faa",
   "metadata": {},
   "source": [
    "### Neural networks require fixed-size input sequences. If your sequences have varying lengths, you need to either truncate longer sequences or pad shorter ones to a common length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23b6ebc8-2be0-43b0-9c75-c5bdc67ebafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15738699-19b0-49c7-867a-5048f9cec35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I Love my dog\",\n",
    "    \"I Love my cat\",\n",
    "    \"You Love my dog very much\",\n",
    "    \"Do you think my dog is awesome\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "995d256d-153e-41d2-8957-6d6cf8255e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Love my dog',\n",
       " 'I Love my cat',\n",
       " 'You Love my dog very much',\n",
       " 'Do you think my dog is awesome']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fe9f706-5f29-46bd-8ad9-8846721c25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100,oov_token=\"<oov>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9eee862-7789-45f6-9740-fd30f4f63e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba461906-256f-4f9b-9769-6b4c43125456",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d6eb6f8-af15-4fdf-8368-d95552565ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<oov>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'very': 8,\n",
       " 'much': 9,\n",
       " 'do': 10,\n",
       " 'think': 11,\n",
       " 'is': 12,\n",
       " 'awesome': 13}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2540dbaa-d3e6-4e8a-a224-fa5141f6aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa9d6593-044a-4d4b-bd37-0a826e641bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4, 8, 9], [10, 6, 11, 2, 4, 12, 13]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ff56c48-8563-49f6-b69c-36b84d28b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b33f0d63-5beb-42a2-ad23-32f916e67a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  5,  3,  2,  4],\n",
       "       [ 0,  0,  0,  5,  3,  2,  7],\n",
       "       [ 0,  6,  3,  2,  4,  8,  9],\n",
       "       [10,  6, 11,  2,  4, 12, 13]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f978415-0456-41cf-9af0-427750a1e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded1 = pad_sequences(sequences,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bb0cb0f-5780-496f-a80f-12801472c22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  2,  4,  0,  0,  0],\n",
       "       [ 5,  3,  2,  7,  0,  0,  0],\n",
       "       [ 6,  3,  2,  4,  8,  9,  0],\n",
       "       [10,  6, 11,  2,  4, 12, 13]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0860dbf6-60e0-4f46-864a-e86812835853",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded2 = pad_sequences(sequences,truncating=\"pre\",maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cbddd54-e7f0-441e-a967-6ba5e029e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  5,  3,  2,  4],\n",
       "       [ 0,  5,  3,  2,  7],\n",
       "       [ 3,  2,  4,  8,  9],\n",
       "       [11,  2,  4, 12, 13]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "236258e6-880d-4cfa-9afc-c87e13aafa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded3 = pad_sequences(sequences,truncating=\"post\",maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1491670d-855c-4221-909c-e9674477412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  5,  3,  2,  4],\n",
       "       [ 0,  5,  3,  2,  7],\n",
       "       [ 6,  3,  2,  4,  8],\n",
       "       [10,  6, 11,  2,  4]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a4565-571a-44f3-b897-0a8ec327922d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
